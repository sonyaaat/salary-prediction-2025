# -*- coding: utf-8 -*-
"""Копія записника "2025_diploma_results"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-oJLmKHTJ2OpEzRjgAOOyYMsxjdXP-u
"""

pip install rapidfuzz

pip install --upgrade gradio

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

"""# 2023 December"""

path = "/content/drive/MyDrive/diploma/2023_dec_raw.csv"
dou_2023_dec= pd.read_csv(path, sep=',')
dou_2023_dec .info()

dou_2023_dec.rename(columns={
    'Timestamp': 'timestamp',
    'Ваша основна зайнятість в ІТ зараз...': 'employmentType',
    'Зарплата / дохід у $$$ за місяць, лише ставка \nЧИСТИМИ - після сплати податків': 'salary',
    'Оберіть вашу основну посаду': 'mainPosition',
    'Ваш тайтл на цій посаді': 'jobTitle',
    'Вкажіть вашу спеціалізацію - Game Design': 'specializationGameDesign',
    'Вкажіть вашу спеціалізацію - Sound': 'specializationSound',
    'Вкажіть вашу спеціалізацію - QA': 'specializationQA',
    'Оберіть вашу посаду - Design': 'positionDesign',
    'GameDev розробники - оберіть вашу спеціалізацію': 'specializationGameDev',
    'Ваша посада - Marketing': 'positionMarketing',
    'Оберіть вашу посаду - HR': 'positionHR',
    'Ваша посада - PM': 'positionPM',
    'Ваша посада - Analyst': 'positionAnalyst',
    'Ваша посада - DS': 'positionDS',
    'Ваша посада - DevOps': 'positionDevOps',
    'Ваша посада - Top management': 'positionTopManagement',
    'Ваша спеціалізація - Support': 'specializationSupport',
    'Чи використовуєте ви у своїй роботі мови програмування (одну чи декілька)?': 'usesProgrammingLanguages',
    'Основна мова програмування': 'mainProgrammingLanguage',
    'Вкажіть вашу основну спеціалізацію': 'mainSpecialization',
    'В якій сфері проєкт, в якому ви зараз працюєте?': 'projectDomain',
    'Основний напрям роботи компанії, в якій працюєте': 'companyMainArea',
    'Кількість спеціалістів у вашій компанії (в Україні/з України)': 'companySizeUA',
    'Загальний стаж роботи за нинішньою ІТ-спеціальністю': 'experience',
    'Яка у вас освіта?': 'educationLevel',
    'Знання англійської мови': 'englishProficiency',
    'Де ви зараз живете? ': 'currentLocation',
    'В якій області ви зараз живете?': 'currentRegion',
    'Ваша стать': 'gender',
    'Ваш вік': 'age'
}, inplace=True)

dou_2023_dec.head()

dou_2023_dec.info()

"""# 2024 June"""

path = "/content/drive/MyDrive/diploma/2024_june_raw.csv"
dou_2024= pd.read_csv(path, sep=',')
dou_2024.head()

dou_2024.info()

dou_2024.rename(columns={
    'Timestamp': 'timestamp',
    'Ваша основна зайнятість в ІТ зараз...': 'employmentType',
    'Зарплата / дохід в ІТ у $$$ за місяць, лише ставка \nЧИСТИМИ - після сплати податків': 'salary',
    'Оберіть вашу основну посаду': 'mainPosition',
    'Ваш тайтл на цій посаді': 'jobTitle',
    'Вкажіть вашу спеціалізацію - Game design': 'specializationGameDesign',
    'Вкажіть вашу спеціалізацію - Sound': 'specializationSound',
    'Вкажіть вашу спеціалізацію - QA': 'specializationQA',
    'Оберіть вашу посаду - Design': 'positionDesign',
    'Якщо ви працюєте в GameDev,  оберіть вашу спеціалізацію': 'specializationGameDev',
    'Ваша посада - Marketing': 'positionMarketing',
    'Оберіть вашу посаду - HR': 'positionHR',
    'Ваша посада - PM': 'positionPM',
    'Ваша посада - Analyst': 'positionAnalyst',
    'Ваша посада - DS': 'positionDS',
    'Ваша посада- DevOps': 'positionDevOps',
    'Ваша посада - Top management': 'positionTopManagement',
    'Ваша спеціалізація - Support': 'specializationSupport',
    'Чи використовуєте ви у своїй роботі мови програмування (одну чи декілька)?': 'usesProgrammingLanguages',
    'Основна мова програмування': 'mainProgrammingLanguage',
    'Вкажіть вашу основну спеціалізацію': 'mainSpecialization',
    'В якій сфері проєкт, в якому ви зараз працюєте?': 'projectDomain',
    'Основний напрям роботи компанії, в якій працюєте': 'companyMainArea',
    'Кількість спеціалістів у вашій компанії (з України)': 'companySizeUA',
    'Загальний стаж роботи за нинішньою ІТ-спеціальністю': 'experience',
    'Яка у вас освіта?': 'educationLevel',
    'Знання англійської мови': 'englishProficiency',
    'Де живуть': 'currentLocation',
    'В якій області ви зараз живете?': 'currentRegion',
    'Ваша стать': 'gender',
    'Ваш вік (повних років)': 'age'
}, inplace=True)

columns_to_drop = ['Ваша посада - C-level', 'Оберіть вашу посаду - Art and Animation']
dou_2024 = dou_2024.drop(columns=columns_to_drop)

"""# 2023 June"""

path = "/content/drive/MyDrive/diploma/2023_june_raw.csv"
dou_2023_june= pd.read_csv(path, sep=';')
dou_2023_june.info()

dou_2023_june.rename(columns={
    'Timestamp': 'timestamp',
    'Ваша основна зайнятість в ІТ зараз...': 'employmentType',
    'Зарплата / дохід у $$$ за місяць, лише ставка після сплати податків': 'salary',
    'Спеціалізація': 'mainPosition',#???
    'Ваш тайтл на цій посаді': 'jobTitle',
    'Вкажіть вашу спеціалізацію Game Design': 'specializationGameDesign',
    'Вкажіть вашу спеціалізацію Sound': 'specializationSound',
    'Вкажіть вашу спеціалізацію - QA': 'specializationQA',
    'Оберіть вашу посаду Designer / Artist': 'positionDesign',
    'Якщо ви працюєте в GameDev,  оберіть вашу спеціалізацію': 'specializationGameDev',
    'Ваша посада Marketing': 'positionMarketing',
    'Оберіть вашу посаду HR': 'positionHR',
    'Ваша посада - PM': 'positionPM',
    'Ваша посада Analyst': 'positionAnalyst',
    'Ваша посада Data Science': 'positionDS',
    'Ваша посада DevOps, SRE': 'positionDevOps',
    'Ваша посада Management': 'positionTopManagement',
    'Ваша спеціалізація Support': 'specializationSupport',
    'Чи використовуєте ви у своїй роботі мови програмування (одну чи декілька)?': 'usesProgrammingLanguages',
    'Основна мова програмування': 'mainProgrammingLanguage',
    'Вкажіть вашу основну спеціалізацію': 'mainSpecialization',
    'В якій сфері проєкт, в якому ви зараз працюєте?': 'projectDomain',
    'Основний напрям роботи компанії, в якій працюєте': 'companyMainArea',
    'Кількість спеціалістів у вашій компанії (в Україні/з України)': 'companySizeUA',
    'Загальний стаж роботи за нинішньою ІТ-спеціальністю': 'experience',
    'Яка у вас освіта?': 'educationLevel',
    'Знання англійської мови': 'englishProficiency',
    'Де ви зараз живете? ': 'currentLocation',
    'В якій області ви зараз живете?': 'currentRegion',
    'Ваша стать': 'gender',
    'Ваш вік': 'age'
}, inplace=True)
dou_2023_june.info()

columns_to_drop = ['Ваша посада CTO, Director']
dou_2023_june = dou_2023_june.drop(columns=columns_to_drop)

dou_2023_june['salary'] = pd.to_numeric(dou_2023_june['salary'], errors='coerce')
dou_2023_june['age'] = pd.to_numeric(dou_2023_june['age'], errors='coerce')

"""# Merging datasets"""

print(dou_2023_dec.columns)
print(dou_2024.columns)
print(dou_2023_june.columns)

columns_dec = set(dou_2023_dec.columns)
columns_2024 = set(dou_2024.columns)
columns_june = set(dou_2023_june.columns)


common_columns = columns_dec & columns_2024 & columns_june
print(f"Common columns: {common_columns}")

unique_dec = columns_dec - common_columns
unique_2024 = columns_2024 - common_columns
unique_june = columns_june - common_columns


print(f"Unique columns in dou_2023_dec: {unique_dec}")
print(f"Unique columns in dou_2024: {unique_2024}")
print(f"Unique columns in dou_2023_june: {unique_june}")

required_columns = [
    'projectDomain',
    'currentLocation',
    'mainSpecialization',
    'englishProficiency', 'companySizeUA', 'age',
    'gender', 'salary',
    'companyMainArea', 'experience',
    'mainPosition', 'currentRegion', 'employmentType',
    'educationLevel',
    'jobTitle'
]

# Selecting the required columns from each dataset
dou_2023_dec_filtered = dou_2023_dec[required_columns]
dou_2024_filtered = dou_2024[required_columns]
dou_2023_june_filtered = dou_2023_june[required_columns]

# Combining datasets
combined_data = pd.concat([dou_2023_dec_filtered, dou_2024_filtered,
                           dou_2023_june_filtered], ignore_index=True)

combined_data.info()

# Adding a data source
# dou_2023_dec_filtered['source'] = '2023_dec'
# dou_2024_filtered['source'] = '2024_june'
# dou_2023_june_filtered['source'] = '2023_june'

combined_data.head()

# experience to int
combined_data['experience'] = pd.to_numeric(combined_data['experience'], errors='coerce').fillna(0).astype(int)

print(combined_data.isnull().sum())

# Check for duplicates
print(f"Number of duplicates: {combined_data.duplicated().sum()}")

# Remove duplicates
combined_data.drop_duplicates(inplace=True)
print(f"Dataset Shape after removing duplicates: {combined_data.shape}")

print(combined_data.isnull().sum())

"""# Сorrelation matrix"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

columns_to_encode = [
    'projectDomain', 'currentLocation', 'mainSpecialization', 'companyMainArea',
    'currentRegion', 'employmentType', 'jobTitle', 'mainPosition', 'englishProficiency',
    'companySizeUA', 'educationLevel', 'gender', 'experience','age'
]

combined_data2=combined_data.copy()

for column in columns_to_encode:
    combined_data2[column] = label_encoder.fit_transform(combined_data2[column])


combined_data2.head()

import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = combined_data2.corr()


plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", cbar=True, square=True)
plt.title('Кореляційна матриця')
plt.show()

"""# Vizualizations"""

# Summary statistics for numerical features
combined_data[['age', 'salary','experience']].describe()

# Вибір числових стовпців
numerical_features = combined_data.select_dtypes(include=['float64', 'int64'])

# Побудова діаграм розподілу для кожної числової ознаки
plt.figure(figsize=(12, 10))
for i, column in enumerate(numerical_features.columns):
    plt.subplot(2, 2, i + 1)
    sns.kdeplot(numerical_features[column], shade=True, color='skyblue')
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Density')

plt.tight_layout()
plt.show()

# Вибір числових стовпців
numerical_features = combined_data.select_dtypes(include=['float64', 'int64'])

# Обчислення кореляційної матриці
correlation_matrix = numerical_features.corr()

# Побудова теплової карти
plt.figure(figsize=(10, 8))
sns.heatmap(
    correlation_matrix,
    annot=True,  # Додає значення в клітини
    cmap='coolwarm',  # Колірна палетка
    fmt='.2f',  # Формат чисел
    linewidths=0.5,  # Ширина ліній між клітинами
    linecolor='black'  # Колір ліній між клітинами
)

plt.title('Correlation Heatmap of Numerical Features', fontsize=18)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.histplot(combined_data['age'], kde=True, ax=axes[0])
axes[0].set_title('Age Distribution')

sns.histplot(combined_data['salary'], kde=True, ax=axes[1])
axes[1].set_title('Salary Distribution')

plt.show()

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.boxplot(y=combined_data['age'], ax=axes[0])
axes[0].set_title('Age Boxplot')

sns.boxplot(y=combined_data['salary'], ax=axes[1])
axes[1].set_title('Salary Boxplot')

plt.show()

# List of categorical columns
categorical_columns = ['projectDomain', 'currentLocation', 'mainSpecialization', 'englishProficiency',
                       'companySizeUA', 'gender', 'companyMainArea', 'experience', 'mainPosition',
                       'currentRegion', 'employmentType', 'educationLevel', 'jobTitle']

# Print value counts for each categorical feature
for col in categorical_columns:
    print(f"Value counts for {col}:")
    print(combined_data[col].value_counts())
    print("-" * 50)

# Compute correlation matrix
corr_matrix = combined_data[['age', 'salary']].corr()

# Plot heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Plot histogram and boxplot for salary
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.histplot(combined_data['salary'], kde=True, ax=axes[0])
axes[0].set_title('Salary Distribution')

sns.boxplot(y=combined_data['salary'], ax=axes[1])
axes[1].set_title('Salary Boxplot')

plt.show()

"""#Preprocessing"""

# Function for removing anomalies by salary
def remove_salary_outliers(data, salary_col, group_col):
    cleaned_data = []
    for group, subset in data.groupby(group_col):
        Q1 = subset[salary_col].quantile(0.25)
        Q3 = subset[salary_col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 0.8 * IQR
        upper_bound = Q3 + 0.8 * IQR
        cleaned_subset = subset[(subset[salary_col] >= lower_bound) & (subset[salary_col] <= upper_bound)]
        cleaned_data.append(cleaned_subset)
    return pd.concat(cleaned_data)

categorical_columns = ['projectDomain', 'currentLocation', 'mainSpecialization', 'englishProficiency',
                       'companySizeUA', 'gender', 'companyMainArea', 'experience', 'mainPosition',
                       'currentRegion', 'employmentType', 'educationLevel', 'jobTitle']

"""##projectDomain"""

combined_data['projectDomain'].nunique()

# Replace 'Other' value in the 'projectDomain' column
combined_data['projectDomain'] = combined_data['projectDomain'].replace('Інша', 'Other')

domains = combined_data['projectDomain'].value_counts()
domains

combined_data['projectDomain'].value_counts().head(25)

combined_data['projectDomain'].isna().sum()

combined_data = combined_data.dropna(subset=['projectDomain'])
unique_domains = combined_data['projectDomain'].unique()

print(combined_data.isnull().sum())

# groups similar project domains based on string similarity using the rapidfuzz library
import pandas as pd
from rapidfuzz import process, fuzz

# Get unique domain names
unique_domains = combined_data['projectDomain'].dropna().unique()

# Set the similarity threshold
similarity_threshold = 40

# Create a dictionary to map each domain to its group
domain_groups = {}

# Dictionary for grouping
domain_mapping = {}

# Iterate through each domain and find the most similar ones
for domain in unique_domains:
    if domain_groups:  # Ensure there is something to compare with
        match = process.extractOne(domain, domain_groups.keys(), scorer=fuzz.ratio)
    else:
        match = None  # If there are no groups yet, leave it as None

    if match is not None and match[1] >= similarity_threshold:
        domain_groups[match[0]].append(domain)
        domain_mapping[domain] = match[0]  # Assign the group
    else:
        domain_groups[domain] = [domain]
        domain_mapping[domain] = domain  # Assign as a new group independently


combined_data['projectDomain'] = combined_data['projectDomain'].map(domain_mapping)

category_counts = combined_data['projectDomain'].value_counts()

# create a list of categories with less than 200 values
small_categories = category_counts[category_counts < 200].index.tolist()
small_categories

# Replace these categories with "Other"
combined_data['projectDomain'] = combined_data['projectDomain'].apply(
    lambda x: 'Other' if x in small_categories else x
)

category_counts = combined_data['projectDomain'].value_counts()

category_counts

print(len(category_counts))

# I don't use it now, I replaced it with rapidfuzz

# threshold = 0.005 * len(combined_data)
# print(f"Поріг для об'єднання: {threshold}")

# projectDomain_counts = combined_data['projectDomain'].value_counts()

# rare_projectDomains = projectDomain_counts[projectDomain_counts < threshold].index
# print(f"Кількість рідкісних доменів: {len(rare_projectDomains)}")
# print(f"Рідкісні домени: {rare_projectDomains}")


# combined_data['projectDomain'] = combined_data['projectDomain'].apply(
#     lambda x: 'Other' if x in rare_projectDomains else x
# )

# updated_projectDomain_counts = combined_data['projectDomain'].value_counts()
# print(updated_projectDomain_counts)


# plt.figure(figsize=(10, 6))
# sns.barplot(y=updated_projectDomain_counts.index, x=updated_projectDomain_counts.values, orient='h')
# plt.title('Оновлений розподіл проектних доменів')
# plt.xlabel('Кількість')
# plt.ylabel('Проектний домен')
# plt.show()

# Removal of anomalies
combined_data = remove_salary_outliers(combined_data, 'salary', 'projectDomain')

combined_data.columns

"""##mainSpecialization"""

print(combined_data.isnull().sum())

unique_specializations = combined_data['mainSpecialization'].unique()

print(unique_specializations)

len(unique_specializations)

# Normalize the values ​​in the 'mainSpecialization' column
combined_data['mainSpecialization'] = combined_data['mainSpecialization'].str.strip().str.lower()

# Combining similar values
specialization_mapping = {
    'back-end розробка': 'Back-end розробка',
    'back-end  розробка': 'Back-end розробка',
    'front-end розробка': 'Front-end розробка',
    'front-end  розробка': 'Front-end розробка',
    'mobile розробка': 'Mobile розробка',
    'mobile  розробка': 'Mobile розробка',
    'embedded': 'Embedded',
    'embedded ': 'Embedded',
    'embedded  ': 'Embedded',
    'інше':'Other'
}
combined_data['mainSpecialization'] = combined_data['mainSpecialization'].replace(specialization_mapping)

combined_data['mainSpecialization'].unique()

#get the number of values ​​for each specialization and sort in descending order
specialization_counts = combined_data['mainSpecialization'].value_counts().sort_values(ascending=False)

# Вивести результат
specialization_counts

specialization_counts.head(25)

# # Defining the top popular values ​​(for example, top 9)
# top_specializations = specialization_counts.head(8)


# # Replace all other values ​​with "other"
# combined_data['mainSpecialization'] = combined_data['mainSpecialization'].apply(
#     lambda x: x if x in top_specializations else 'Other'
# )

# # Getting updated results
# updated_specialization_counts = combined_data['mainSpecialization'].value_counts()
# print(updated_specialization_counts)

# updated_specialization_counts

specialization_mapping_to_english = {
    'Back-end розробка': 'Back-end Development',
    'Front-end розробка': 'Front-end Development',
    'full stack розробка': 'Full Stack Development',
    'qa': 'QA',
    'Mobile розробка': 'Mobile Development',
    'робота з даними, аналіз даних': 'Data Engineering & Analytics',
    'devops': 'DevOps',
    'Other': 'Other'
}

combined_data['mainSpecialization'] = combined_data['mainSpecialization'].replace(specialization_mapping_to_english)

updated_specialization_counts = combined_data['mainSpecialization'].value_counts()
print(updated_specialization_counts)

updated_specialization_counts

# Visualization
plt.figure(figsize=(12, 6))
sns.barplot(x=updated_specialization_counts.index, y=updated_specialization_counts.values, palette="viridis")
plt.title('Розподіл спеціалізацій', fontsize=16)
plt.xlabel('Спеціалізація', fontsize=14)
plt.ylabel('Кількість', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

combined_data = remove_salary_outliers(combined_data, 'salary', 'mainSpecialization')

combined_data['mainSpecialization'].value_counts()

"""##mainPosition"""

print(combined_data['mainPosition'].value_counts())

len(combined_data['mainPosition'].unique())

# groups similar project domains based on string similarity using the rapidfuzz library

from rapidfuzz import process, fuzz
import pandas as pd

unique_specializations = combined_data['mainPosition'].dropna().unique()

similarity_threshold = 65

specialization_groups = {}

for specialization in unique_specializations:
    match = process.extractOne(specialization, specialization_groups.keys(), scorer=fuzz.ratio)

    if match:
        matched_value, score, _ = match
        if score >= similarity_threshold:
            specialization_groups[matched_value].append(specialization)
        else:
            specialization_groups[specialization] = [specialization]
    else:
        specialization_groups[specialization] = [specialization]

specialization_mapping = {}
for main_category, similar_specializations in specialization_groups.items():
    for spec in similar_specializations:
        specialization_mapping[spec] = main_category

combined_data['mainPosition'] = combined_data['mainPosition'].map(specialization_mapping)

category_counts = combined_data['mainPosition'].value_counts()
print(category_counts)

# I don't use it now, I replaced it with rapidfuzz


# combined_data['mainPosition'] = combined_data['mainPosition'].str.strip().str.lower()


# position_mapping = {
#     'software engineer / programmer': 'software engineer',
#     'qa / aqa engineer (junior, middle, senior, team/tech lead, manager)': 'qa engineer',
#     'qa / aqa / qc engineer (junior, middle, senior, team/tech lead, manager)': 'qa engineer',
#     'project/product/program/delivery/engineering manager, product owner, producer, scrum master': 'project/product manager',
#     'devops, sre': 'devops',
#     'devops, sre, operations': 'devops',
#     'designer / artist (graphic, video, ux/ui)': 'designer',
#     'design and ui/ux (graphic, video, ux/ui)': 'designer',
#     'сto, director of engineering, program director, coo, ceo, (co-)founder': 'сto/director/ceo',
#     'сto, director of engineering, program director, coo, ceo, (co-)founder, head of r&d': 'сto/director/ceo',
#     'sales / business development': 'sales/business development',
#     'sales / business development / lead generation': 'sales/business development',
#     'design (game, level, narrative)': 'game design',
#     'game design (game, level, narrative)': 'game design',
#     'support (customer, technical, community)': 'support',
#     'marketing, pr, seo, copywriter': 'marketing',
#     'data science, machine learning, ai, big data, data engineer': 'data science/ai',
#     'hr/recruiter/learning and development': 'hr/recruiter',
#     'analyst (business, data, system etc)': 'analyst',
#     'technical writer': 'technical writer',
#     'html coder': 'html coder',
#     'customer success': 'customer success',
#     'finance /accounting /internal audit': 'finance/accounting',
#     'dba': 'dba',
#     'legal': 'legal',
#     'hardware engineer': 'hardware engineer',
#     'office manager': 'office manager',
#     'office manager / support': 'office manager',
#     'sound specialist / designer': 'sound specialist',
#     'localization specialist / translator': 'localization specialist',
#     'security specialist': 'security specialist',
#     'artist / animator': 'artist/animator',
#     'erp/crm': 'erp/crm',
#     'sysadmin': 'sysadmin',
#      'інше':'Other'}


# combined_data['mainPosition'] = combined_data['mainPosition'].replace(position_mapping)

# additional_mapping = {
#     # Technical/Content Roles
#     'technical writer': 'technical/content specialist',
#     'html coder': 'technical/content specialist',
#     'localization specialist': 'technical/content specialist',

#     # Creative Roles
#     'sound specialist': 'creative/design',
#     'artist/animator': 'creative/design',
#     'game design': 'creative/design',

#     # Infrastructure Roles
#     'dba': 'infrastructure specialist',
#     'sysadmin': 'infrastructure specialist',
#     'hardware engineer': 'infrastructure specialist',

#     # Customer Support Roles
#     'customer success': 'customer support',
#     'support': 'customer support',

#     # Administrative Roles
#     'finance/accounting': 'finance/accounting',  # Keep separate (specialized)
#     'legal': 'legal',  # Keep separate (specialized)
#     'office manager': 'administrative',  # General administrative role
# }
# combined_data['mainPosition'] = combined_data['mainPosition'].replace(additional_mapping)

len(combined_data['mainPosition'].unique())

print(combined_data['mainPosition'].value_counts())

position_counts = combined_data['mainPosition'].value_counts()
insignificant_positions = position_counts[position_counts < 200].index

# Replace minor values ​​with "other"
combined_data['mainPosition'] = combined_data['mainPosition'].apply(
    lambda x: 'Other' if x in insignificant_positions else x
)


updated_position_counts = combined_data['mainPosition'].value_counts()

updated_position_counts

# Vizualization
plt.figure(figsize=(12, 6))
sns.barplot(x=updated_position_counts.index, y=updated_position_counts.values, palette="viridis")
plt.title('Розподіл посад', fontsize=16)
plt.xlabel('Посада', fontsize=14)
plt.ylabel('Кількість', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()


for i, value in enumerate(updated_position_counts.values):
    plt.text(i, value + 50, str(value), ha='center', va='bottom', fontsize=10)

combined_data = remove_salary_outliers(combined_data, 'salary', 'mainPosition')

"""##englishProficiency"""

# A dictionary for combining similar categories
proficiency_mapping = {
    'Upper-Intermediate': 'Upper-Intermediate',
    '5. Upper-Intermediate': 'Upper-Intermediate',
    'Intermediate': 'Intermediate',
    '4. Intermediate': 'Intermediate',
    'Advanced': 'Advanced',
    '6. Advanced': 'Advanced',
    'Pre-Intermediate': 'Pre-Intermediate',
    '3. Pre-Intermediate': 'Pre-Intermediate',
    'Elementary': 'Elementary',
    '2. Elementary': 'Elementary',
    'Не знаю взагалі': 'No Proficiency',
    '1. Не знаю взагалі': 'No Proficiency'
}


combined_data['englishProficiency'] = combined_data['englishProficiency'].map(proficiency_mapping)

"""##currentLocation"""

location_mapping = {
    'За кордоном (переїхали через війну, але планую повернутися)': 'Abroad (plan to return)',
    'За кордоном, але планую повернутися': 'Abroad (plan to return)',
    'Планують повернутися': 'Abroad (plan to return)',
    'В Україні': 'In Ukraine'
}

combined_data['currentLocation'] = combined_data['currentLocation'].map(location_mapping)

"""##company_size"""

company_size_mapping = {
    'до 200': 'Up to 200',
    '4. до 200': 'Up to 200',
    'понад 1000': 'Over 1000',
    '6. понад 1000': 'Over 1000',
    'до 50': 'Up to 50',
    '3. до 50': 'Up to 50',
    'до 1000': 'Up to 1000',
    '5. до 1000': 'Up to 1000',
    'до 10 спеціалістів': 'Up to 10 specialists',
    '2. до 10 спеціалістів': 'Up to 10 specialists',
    'Лише я / фріланс': 'Only me / Freelance',
    '1. Лише я / фріланс': 'Only me / Freelance'
}

combined_data['companySizeUA'] = combined_data['companySizeUA'].map(company_size_mapping)

"""##company_main_area"""

company_main_area_mapping = {
    'Продуктова': 'Product',
    'Аутсорсингова': 'Outsourcing',
    'Сервісна (аутсорсингова)': 'Outsourcing',
    'Аутстафінгова': 'Outstaffing',
    'Стартап': 'Startup',
    'Інша': 'Other',
    'Фріланс': 'Freelance',
    'Державне підприємство / установа': 'Government Entity'
}

combined_data['companyMainArea'] = combined_data['companyMainArea'].map(company_main_area_mapping)

"""##region_mapping"""

region_mapping = {
    # Western Ukraine
    'Львів чи область': 'Western Ukraine',
    'Івано-Франківськ чи область': 'Western Ukraine',
    'Тернопіль чи область': 'Western Ukraine',
    'Ужгород чи Закарпатська область': 'Western Ukraine',
    'Чернівці чи область': 'Western Ukraine',
    'Рівне чи область': 'Western Ukraine',
    'Луцьк чи Волинська область': 'Western Ukraine',

    # Central Ukraine
    'Київ чи область': 'Central Ukraine',
    'Вінниця чи область': 'Central Ukraine',
    'Черкаси чи область': 'Central Ukraine',
    'Полтава чи область': 'Central Ukraine',
    'Хмельницький чи область': 'Central Ukraine',
    'Житомир чи область': 'Central Ukraine',
    'Кропивницький чи область': 'Central Ukraine',

    # Eastern Ukraine
    'Харків чи область': 'Eastern Ukraine',
    'Дніпро чи область': 'Eastern Ukraine',
    'Запоріжжя чи область': 'Eastern Ukraine',
    'Суми чи область': 'Eastern Ukraine',
    'Чернігів чи область': 'Eastern Ukraine',

    # Southern Ukraine
    'Одеса чи область': 'Southern Ukraine',
    'Миколаїв чи область': 'Southern Ukraine',
    'Херсон чи область': 'Southern Ukraine',

    # Other
    'Інше': 'Other',
    'Донецька чи Луганська область': 'Other',
    'Не в Україні': 'Other',
}



# region_mapping = {
#     'Київ чи область': 'Kyiv or region',
#     'Львів чи область': 'Lviv or region',
#     'Дніпро чи область': 'Dnipro or region',
#     'Одеса чи область': 'Odesa or region',
#     'Харків чи область': 'Kharkiv or region',
#     'Івано-Франківськ чи область': 'Ivano-Frankivsk or region',
#     'Вінниця чи область': 'Vinnytsia or region',
#     'Черкаси чи область': 'Cherkasy or region',
#     'Полтава чи область': 'Poltava or region',
#     'Хмельницький чи область': 'Khmelnytskyi or region',
#     'Тернопіль чи область': 'Ternopil or region',
#     'Ужгород чи Закарпатська область': 'Uzhhorod or Zakarpattia region',
#     'Чернівці чи область': 'Chernivtsi or region',
#     'Рівне чи область': 'Rivne or region',
#     'Житомир чи область': 'Zhytomyr or region',
#     'Луцьк чи Волинська область': 'Lutsk or Volyn region',
#     'Кропивницький чи область': 'Kropyvnytskyi or region',
#     'Суми чи область': 'Sumy or region',
#     'Запоріжжя чи область': 'Zaporizhzhia or region',
#     'Чернігів чи область': 'Chernihiv or region',
#     'Миколаїв чи область': 'Mykolaiv or region',
#     'Інше': 'Other',
#     'Донецька чи Луганська область': 'Donetsk or Luhansk region',
#     'Херсон чи область': 'Kherson or region',
#     'Не в Україні': 'Not in Ukraine'
# }

combined_data['currentRegion'] = combined_data['currentRegion'].map(region_mapping)

"""##gender"""

gender_mapping = {
    'Чоловік': 'Male',
    'Жінка': 'Female'
}

combined_data['gender'] = combined_data['gender'].map(gender_mapping)

"""##employmentType"""

employment_type_mapping = {
    'Працюю full-time в ІТ-компанії чи ІТ-відділі': 'Full-time IT employee',
    'Втратив(-ла) роботу в ІТ і шукаю нову': 'Unemployed (looking for IT job)',
    'Працюю part-time в ІТ-компанії чи ІТ-відділі': 'Part-time IT employee',
    'Я фрилансер(-ка) в ІТ': 'Freelancer in IT',
    'Зараз на повністю оплачуваному бенчі в ІТ-компанії': 'On bench (paid/unpaid)',
    'Зараз в неоплачуваній відпустці / бенчі в ІТ-компанії': 'On bench (paid/unpaid)',
    'Зараз на частково оплачуваному бенчі в ІТ-компанії': 'On bench (paid/unpaid)',
    'Тимчасово не працюю і не шукаю роботу (sabbatical, декрет, волонтерство etc)': 'Not working (temporary)',
    'Працював(-ла) в IT, нині на військовій службі': 'Military service (ex-IT)'
}

combined_data['employmentType'] = combined_data['employmentType'].map(employment_type_mapping)

"""##educationLevel"""

education_level_mapping = {
    'Вища (бакалавр, спеціаліст, магістр) - одна або декілька': 'Higher Education (Bachelor, Specialist, Master)',
    '5. Вища (бакалавр, спеціаліст, магістр) - одна або декілька': 'Higher Education (Bachelor, Specialist, Master)',
    'Незакінчена вища': 'Incomplete Higher Education',
    '4. Незакінчена вища': 'Incomplete Higher Education',
    'Ще студент вишу': 'Current University Student',
    '3. Ще студент вишу': 'Current University Student',
    'Середня спеціальна': 'Secondary Specialized Education',
    '2. Середня спеціальна': 'Secondary Specialized Education',
    'Середня': 'Secondary Education',
    '1. Середня': 'Secondary Education',
    'Науковий ступінь (кандидат / доктор наук, PhD)': 'PhD or Doctorate',
    '6. Науковий ступінь (кандидат / доктор наук, PhD)': 'PhD or Doctorate'
}

combined_data['educationLevel'] = combined_data['educationLevel'].map(education_level_mapping)

combined_data = remove_salary_outliers(combined_data, 'salary', 'educationLevel')

print(combined_data['salary'].describe())

"""##jobTitle"""

print(combined_data['jobTitle'].value_counts())

job_title_mapping = {
    "Junior": "Junior",
    "Intern/Trainee": "Junior",
    "Middle": "Middle",
    "Senior": "Senior",
    "Tech Lead": "Senior",
    "Architect": "Senior",
    "Team Lead": "Lead/Manager",
    "Manager": "Lead/Manager",
    "Head": "Lead/Manager",
    "Немає тайтлу": "No Title"
}

combined_data['jobTitle'] = combined_data['jobTitle'].map(job_title_mapping)

print(combined_data['jobTitle'].value_counts())

combined_data = remove_salary_outliers(combined_data, 'salary', 'jobTitle')

print(combined_data['salary'].describe())

print(combined_data['projectDomain'].value_counts())
print("___________________________________________________________")


print(combined_data['mainSpecialization'].value_counts())
print("___________________________________________________________")


print(combined_data['mainPosition'].value_counts())
print("___________________________________________________________")



print(combined_data['currentLocation'].value_counts())
print("___________________________________________________________")


print(combined_data['englishProficiency'].value_counts())
print("___________________________________________________________")


print(combined_data['companySizeUA'].value_counts())
print("___________________________________________________________")


print(combined_data['gender'].value_counts())
print("___________________________________________________________")


print(combined_data['companyMainArea'].value_counts())
print("___________________________________________________________")

print(combined_data['currentRegion'].value_counts())
print("___________________________________________________________")

print(combined_data['mainPosition'].value_counts())
print("___________________________________________________________")

print(combined_data['employmentType'].value_counts())
print("___________________________________________________________")

print(combined_data['educationLevel'].value_counts())
print("___________________________________________________________")


print(combined_data['jobTitle'].value_counts())
print("___________________________________________________________")

"""##Experience"""

print(combined_data['experience'].describe())
print(combined_data['experience'].value_counts())

combined_data['experience'] = pd.cut(
    combined_data['experience'],
    bins=[0, 3, 6, 9, 12, 15],
    labels=['0-3', '4-6', '7-9', '10-12', '13-15'],
    include_lowest=True
)
print(combined_data['experience'].value_counts())

combined_data = remove_salary_outliers(combined_data, 'salary', 'experience')

combined_data.info()

"""##age"""

print(combined_data['age'].value_counts())

bins = [16, 24, 34, 44, 80]
labels = ['16-24', '25-34', '35-44', '45+']
combined_data['age'] = pd.cut(combined_data['age'], bins=bins, labels=labels, right=True)

combined_data = remove_salary_outliers(combined_data, 'salary', 'age')

"""## mainSpecialization other"""

combined_data_copy = combined_data.copy()

print(combined_data_copy['mainSpecialization'].value_counts())

print(combined_data.isnull().sum())

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a copy of the original dataset
combined_data_copy = combined_data.copy()

# Convert categorical features to numerical
label_encoder_position = LabelEncoder()

# Need to convert text values to numerical for "mainPosition"
combined_data_copy['mainPosition2'] = label_encoder_position.fit_transform(combined_data_copy['mainPosition'].fillna('Unknown'))

# Replace "Other" with NaN
combined_data_copy['mainSpecialization'] = combined_data_copy['mainSpecialization'].replace('Other', pd.NA)

# Select rows where mainSpecialization is missing (NaN)
missing_specializations = combined_data_copy[combined_data_copy['mainSpecialization'].isna()]

# Select features for modeling (mainPosition)
features = ['mainPosition2']  # Можна додати більше ознак, якщо є

# 2. Prepare training data
train_data = combined_data_copy.dropna(subset=['mainSpecialization'])

# X — features, y — target variable (mainSpecialization)
X_train = train_data[features]
y_train = train_data['mainSpecialization']

# 3. Split into training and testing sets for model evaluation
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# 4. Modeling: Use RandomForestClassifier for prediction
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 5. Model evaluation
y_pred = model.predict(X_val)
print(f'Accuracy: {accuracy_score(y_val, y_pred)}')

# 6. Predict specializations for missing values
X_missing = missing_specializations[features]
predicted_specializations = model.predict(X_missing)

# 7. Fill missing values in mainSpecialization
combined_data_copy.loc[combined_data_copy['mainSpecialization'].isna(), 'mainSpecialization'] = predicted_specializations

# 8. Check results
print(combined_data_copy['mainSpecialization'].value_counts())

print(combined_data_copy['mainSpecialization'].value_counts())

print(combined_data['mainSpecialization'].value_counts())

combined_data_copy.head(10)

# combined_data=combined_data_copy

# combined_data=combined_data_copy.dropna(columns=['mainPosition2'])

combined_data = remove_salary_outliers(combined_data, 'salary', 'mainSpecialization')

print(combined_data['salary'].describe())

print(combined_data['mainSpecialization'].value_counts())

"""#EDA"""

dou=combined_data.copy()

dou['mainPosition'].value_counts()

dou['mainPosition'].nunique()

main_position_distribution = dou['mainPosition'].value_counts()[:25]

colors = plt.cm.Paired(range(len(main_position_distribution)))

plt.figure(figsize=(5, 5))
main_position_distribution.plot(kind='barh', color=colors)
plt.title('Main Position Distribution', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

jobTitle_distribution = dou['jobTitle'].value_counts()

colors = plt.cm.Paired(range(len(jobTitle_distribution)))

plt.figure(figsize=(8, 8))
jobTitle_distribution.plot(kind='barh', color=colors)
plt.title('Job Title Distribution', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

educationLevel_distribution = dou['educationLevel'].value_counts()

colors = plt.cm.Paired(range(len(educationLevel_distribution)))

plt.figure(figsize=(8, 8))
educationLevel_distribution.plot(kind='barh', color=colors)
plt.title('Education Level Distribution', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)  # Додаємо сітку для зручності
plt.show()

mainSpecialization_distribution = dou['mainSpecialization'].value_counts()

colors = plt.cm.Paired(range(len(mainSpecialization_distribution)))

plt.figure(figsize=(8, 8))
mainSpecialization_distribution.plot(kind='barh', color=colors)
plt.title('Main Specialization Distribution', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

employmentType_distribution = dou['employmentType'].value_counts()

colors = plt.cm.Paired(range(len(employmentType_distribution)))

plt.figure(figsize=(8, 8))
employmentType_distribution.plot(kind='barh', color=colors)
plt.title('Employment Type Distribution', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


employment_type_distribution = dou['currentLocation'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(employment_type_distribution, labels=employment_type_distribution.index, autopct='%1.1f%%', startangle=90)
plt.title('СurrentLocation Distribution')
plt.axis('equal')
plt.show()

gender_distribution = dou['gender'].value_counts()


colors = plt.cm.Paired(range(len(gender_distribution)))


plt.figure(figsize=(10, 10))
plt.pie(
    gender_distribution,
    labels=gender_distribution.index,
    autopct='%1.1f%%',
    startangle=90,
    colors=colors,
    wedgeprops={'edgecolor': 'black'},
    pctdistance=0.85
)

centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.axis('equal')
plt.title('gender', fontsize=20, fontweight='bold', color='darkblue')
plt.show()

companyMainArea_distribution = dou['currentRegion'].value_counts()

colors = plt.cm.Paired(range(len(companyMainArea_distribution)))

plt.figure(figsize=(8, 8))
companyMainArea_distribution.plot(kind='barh', color=colors)
plt.title('Сurrent Region', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

companyMainArea_distribution = dou['companyMainArea'].value_counts()

colors = plt.cm.Paired(range(len(companyMainArea_distribution)))

plt.figure(figsize=(8, 8))
companyMainArea_distribution.plot(kind='barh', color=colors)
plt.title('Company Main Area Distribution', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

companyMainArea_distribution = dou['projectDomain'].value_counts().head(15)

colors = plt.cm.Paired(range(len(companyMainArea_distribution)))

plt.figure(figsize=(8, 8))
companyMainArea_distribution.plot(kind='barh', color=colors)
plt.title('Рroject Domainn', fontsize=18)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Position', fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


def plot_average_salary_by_category(df, category_column, title):
    avg_salary_by_category = df.groupby(category_column)['salary'].mean().sort_values()

    colors = plt.cm.Paired(range(len(avg_salary_by_category)))

    plt.figure(figsize=(10, 8))
    avg_salary_by_category.plot(kind='barh', color=colors)
    plt.title(title, fontsize=18)
    plt.xlabel('Average Salary ($)', fontsize=12)
    plt.ylabel(category_column, fontsize=16)
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    plt.show()

plot_average_salary_by_category(dou, 'mainPosition', 'Average Salary by Main Position')
plot_average_salary_by_category(dou, 'employmentType', 'Average Salary by Employment Type')
plot_average_salary_by_category(dou, 'educationLevel', 'Average Salary by Education Level')

plot_average_salary_by_category(dou, 'jobTitle', 'Average Salary by jobTitle')

plot_average_salary_by_category(dou, 'age', 'Average Salary by age')

plot_average_salary_by_category(dou, 'experience', 'Average Salary by experience')

plot_average_salary_by_category(dou, 'companySizeUA', 'Average Salary by companySizeUA')

"""#Handle missing values"""

print(combined_data.isnull().sum())
print(combined_data.info())

# Fill missing 'age' values with the median
# combined_data['age'].fillna(combined_data['age'].median(), inplace=True)
combined_data['age'].fillna(combined_data['age'].mode()[0], inplace=True)

print(combined_data.isnull().sum())

# Option 1: Drop rows with missing 'salary' values
combined_data.dropna(subset=['salary'], inplace=True)

# Option 2: Fill missing 'salary' values with the median
# combined_data['salary'].fillna(combined_data['salary'].median(), inplace=True)

region_counts = combined_data['currentRegion'].value_counts()

print(region_counts)

# Fill missing 'currentRegion' values with "Unknown"
combined_data['currentRegion'].fillna('Other', inplace=True)

print(combined_data['currentRegion'].value_counts())

"""# ENCODING

##One-Hot Encoding
"""

combined_data

combined_data

#One-Hot Encoding
df_encoded = pd.get_dummies(combined_data, columns=[
     'currentLocation', 'companyMainArea',
    'currentRegion', 'employmentType', 'jobTitle',
     'mainPosition',
     'age','experience'
    , 'projectDomain', 'mainSpecialization'

    # ,'englishProficiency'
    #  ,'companySizeUA','educationLevel','gender'
])

df_encoded.columns

"""##LabelEncoding"""

#LabelEncoding
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

df_encoded['englishProficiency'] = label_encoder.fit_transform(df_encoded['englishProficiency'])
df_encoded['companySizeUA'] = label_encoder.fit_transform(df_encoded['companySizeUA'])
df_encoded['educationLevel'] = label_encoder.fit_transform(df_encoded['educationLevel'])
df_encoded['gender'] = label_encoder.fit_transform(df_encoded['gender'])

print(df_encoded.head())

"""## Outliers detection with  IQR"""

# calculate the first (Q1) and third (Q3) quartiles
Q1 = df_encoded['salary'].quantile(0.25)
Q3 = df_encoded['salary'].quantile(0.75)

# calculate the interquartile range (IQR)
IQR = Q3 - Q1

# determine the limits for the search for outliers
lower_bound = Q1 - 0.7 * IQR
upper_bound = Q3 + 0.7 * IQR

# filter the data, excluding outliers
df_encoded_no_outliers = df_encoded[
    (df_encoded['salary'] >= lower_bound) &
    (df_encoded['salary'] <= upper_bound)
]

# display information about the new data set
print(f"Data size before outlier removal: {df_encoded.shape[0]}")
print(f"Data size after outlier removal: {df_encoded_no_outliers.shape[0]}")

"""## Outliers detection with Z-Score (draft dont use now)"""

# from scipy import stats

# # Фільтруємо дані, виключаючи викиди (Z-Score > 3 або < -3)
# z_scores = stats.zscore(df_encoded['salary'])

# df_encoded_no_outliers = df_encoded[np.abs(z_scores) <= 3]

# # Виводимо інформацію про новий набір даних
# print(f"Розмір даних до видалення викидів: {df_encoded.shape[0]}")
# print(f"Розмір даних після видалення викидів: {df_encoded_no_outliers.shape[0]}")

# # Перевіряємо статистику зарплат
# print(df_encoded_no_outliers['salary'].describe())

"""## Scaling"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_columns = ['salary']

df_encoded[scaled_columns] = scaler.fit_transform(df_encoded[scaled_columns])

"""#PREPROCESSING FUNCTION"""

import pandas as pd
import joblib
from sklearn.preprocessing import LabelEncoder, StandardScaler

def preprocess_data(df, training=True, label_encoders=None, scaler=None, reference_columns=None):
    """
    Preprocess data by applying One-Hot Encoding, Label Encoding, and Standard Scaling.

    Parameters:
    - df (pd.DataFrame): The input DataFrame to preprocess.
    - training (bool): If True, fit encoders and scaler; if False, use existing encoders/scaler.
    - label_encoders (dict): Pre-fitted label encoders (required for inference).
    - scaler (StandardScaler): Pre-fitted scaler (required for inference).
    - reference_columns (list): List of columns from training data to maintain column consistency.

    Returns:
    - df_processed (pd.DataFrame): Preprocessed DataFrame ready for model input.
    - label_encoders (dict): Fitted label encoders (if training).
    - scaler (StandardScaler): Fitted scaler (if training).
    """

    categorical_onehot = [
        'currentLocation', 'companyMainArea', 'currentRegion', 'employmentType',
        'jobTitle', 'mainPosition', 'age', 'experience', 'projectDomain', 'mainSpecialization'
    ]

    categorical_label = ['englishProficiency', 'companySizeUA', 'educationLevel', 'gender']

    numerical_features = ['salary']

    df = df.copy()

    # One-Hot Encoding
    df_encoded = pd.get_dummies(df, columns=categorical_onehot)

    # Label Encoding
    if training:
        label_encoders = {}
        for col in categorical_label:
            le = LabelEncoder()
            df_encoded[col] = le.fit_transform(df[col])
            label_encoders[col] = le
    else:
        for col in categorical_label:
            if col in df_encoded:
                df_encoded[col] = label_encoders[col].transform(df[col])

    # Ensure column consistency (important for inference)
    if reference_columns:
        missing_cols = set(reference_columns) - set(df_encoded.columns)
        for col in missing_cols:
            df_encoded[col] = 0  # Add missing columns with default value 0

        df_encoded = df_encoded[reference_columns]  # Reorder to match training

    # Standard Scaling
    if training:
        scaler = StandardScaler()
        df_encoded[numerical_features] = scaler.fit_transform(df_encoded[numerical_features])
    else:
        df_encoded[numerical_features] = scaler.transform(df_encoded[numerical_features])

    return df_encoded, label_encoders, scaler

df_encoded, label_encoders, scaler = preprocess_data(combined_data)

"""# Splitting into test train validation datasets"""

from sklearn.model_selection import train_test_split

X = df_encoded.drop('salary', axis=1)
y = df_encoded['salary']

# divide the data into a training set and a test and validation set
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=1)

# divide the test and validation set into validation and test
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=1)

print(f"Training set: {X_train.shape}, {y_train.shape}")
print(f"Validation set: {X_val.shape}, {y_val.shape}")
print(f"Test set: {X_test.shape}, {y_test.shape}")

"""#PREDICTIONS"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

def evaluate_model(model, X_val, y_val, scaler):
    y_val_pred = model.predict(X_val)

    y_val_pred_original = scaler.inverse_transform(y_val_pred.reshape(-1, 1))
    y_val_original = scaler.inverse_transform(y_val.values.reshape(-1, 1))

    errors = abs(y_val_original - y_val_pred_original)

    result_df = pd.DataFrame({
        'Predicted Salary': y_val_pred_original.flatten(),
        'Actual Salary': y_val_original.flatten(),
        'Error': errors.flatten()
    })

    top_15_df = result_df.sort_values(by='Error').head(15)

    return top_15_df

"""# RandomForestRegressor"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

params = {
    'n_estimators': 400,
    'min_samples_split': 10,
    'min_samples_leaf': 2,
    'max_features': 0.5,
    'max_depth': None
}

rf_model = RandomForestRegressor(**params)

rf_model.fit(X_train, y_train)

y_val_pred_rf = rf_model.predict(X_val)

y_test_pred_rf = rf_model.predict(X_test)

mse_rf = mean_squared_error(y_test, y_test_pred_rf)
r2_rf = r2_score(y_test, y_test_pred_rf)

print(f"Mean Squared Error: {mse_rf}")
print(f"R^2 Score: {r2_rf}")

print("_____________RandomForestRegressor___________________")

# Prediction for training data
y_train_pred_dt = rf_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred_dt)
print(f"MSE on training data: {mse_train}")

# Prediction for validation data
y_val_pred_dt = rf_model.predict(X_val)
mse_val = mean_squared_error(y_val, y_val_pred_dt)
print(f"MSE on validation data {mse_val}")

# Prediction for test data
y_test_pred_dt = rf_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred_dt)
print(f"MSE on test data {mse_test}")

print("______________________________________")

from sklearn.metrics import r2_score

# Prediction for training data
y_train_pred_dt = rf_model.predict(X_train)
r2_train = r2_score(y_train, y_train_pred_dt)

# Prediction for validation data
y_val_pred_dt = rf_model.predict(X_val)
r2_val = r2_score(y_val, y_val_pred_dt)

# Prediction for test data
y_test_pred_dt = rf_model.predict(X_test)
r2_test = r2_score(y_test, y_test_pred_dt)

print(f"R² MSE on training data {r2_train}")
print(f"R² Score on validation data: {r2_val}")
print(f"R² Score on test data: {r2_test}")

top_15_predictions = evaluate_model(rf_model, X_val, y_val, scaler)


print(top_15_predictions)

# Load encoders and scaler
# label_encoders = joblib.load('label_encoders.pkl')
# scaler = joblib.load('scaler.pkl')
# reference_columns = joblib.load('reference_columns.pkl')


# # Preprocess training data and store encoders, scaler, and reference columns
# df_encoded, label_encoders, scaler = preprocess_data(combined_data, training=True)

reference_columns = df_encoded.columns.tolist()

index=108

# new_data = combined_data.iloc[[1]].copy()
new_data = combined_data.loc[[index]]
# pd.DataFrame([{
#     'currentLocation': 'In Ukraine',
#     'companyMainArea': 'Product',
#     'currentRegion': 'Western Ukraine',
#     'employmentType': 'Full-time IT employee',
#     'jobTitle': 'Junior',
#     'mainPosition': 'Data Science, Machine Learning, AI, Big Data, Data Engineer',
#     'age': '16-24',
#     'experience': '0-3',
#     'projectDomain': 'Cloud Computing, Machine Learning, Uberfication',
#     'mainSpecialization': 'Back-end розробка',
#     'englishProficiency': 'Intermediate',
#     'companySizeUA': 'Up to 50',
#     'educationLevel': 'Current University Student',
#     'gender': 'Male'
# }])
# combined_data.iloc[0]

# reference_columns = [col for col in df_encoded.columns if col != 'salary']

# Preprocess new data
new_data_encoded, _, _ = preprocess_data(new_data, training=False,
                                         label_encoders=label_encoders,
                                         scaler=scaler,
                                         reference_columns=reference_columns)
new_data_encoded = new_data_encoded.drop('salary', axis=1, errors='ignore')
# Predict Salary
predicted_salary = rf_model.predict(new_data_encoded)

# Inverse scale the predicted salary
predicted_salary_scaled = predicted_salary[0]
predicted_salary_original = scaler.inverse_transform([[predicted_salary_scaled]])[0][0]

print(f"Predicted Salary: {predicted_salary_original}")
# Predicted Salary: 2413.732193732194
print(f"Real salary is {combined_data.loc[index, 'salary']}")

"""#UI"""

import pandas as pd

unique_values = {col: combined_data[col].unique().tolist() for col in combined_data.columns}

import gradio as gr
import pandas as pd

# Assuming unique_values is a dictionary of unique values for each column
def predict_salary(currentLocation, companyMainArea, currentRegion, employmentType, jobTitle, mainPosition, age, experience, projectDomain, mainSpecialization, englishProficiency, companySizeUA, educationLevel, gender):
    # Create a DataFrame from the input values
    new_data = pd.DataFrame([{
        'currentLocation': currentLocation,
        'companyMainArea': companyMainArea,
        'currentRegion': currentRegion,
        'employmentType': employmentType,
        'jobTitle': jobTitle,
        'mainPosition': mainPosition,
        'age': age,
        'experience': experience,
        'projectDomain': projectDomain,
        'mainSpecialization': mainSpecialization,
        'englishProficiency': englishProficiency,
        'companySizeUA': companySizeUA,
        'educationLevel': educationLevel,
        'gender': gender
    }])

    # Preprocess the data
    new_data_encoded, _, _ = preprocess_data(new_data, training=False,
                                             label_encoders=label_encoders,
                                             scaler=scaler,
                                             reference_columns=reference_columns)
    new_data_encoded = new_data_encoded.drop('salary', axis=1, errors='ignore')

    # Predict Salary
    predicted_salary = rf_model.predict(new_data_encoded)

    # Inverse scale the predicted salary
    predicted_salary_scaled = predicted_salary[0]
    predicted_salary_original = scaler.inverse_transform([[predicted_salary_scaled]])[0][0]

    return f"Predicted Salary: {predicted_salary_original}"

# Create Gradio interface
inputs = [
    gr.Dropdown(label="Current Location", choices=unique_values['currentLocation']),
    gr.Dropdown(label="Company Main Area", choices=unique_values['companyMainArea']),
    gr.Dropdown(label="Current Region", choices=unique_values['currentRegion']),
    gr.Dropdown(label="Employment Type", choices=unique_values['employmentType']),
    gr.Dropdown(label="Job Title", choices=unique_values['jobTitle']),
    gr.Dropdown(label="Main Position", choices=unique_values['mainPosition']),
    gr.Dropdown(label="Age", choices=unique_values['age']),
    gr.Dropdown(label="Experience", choices=unique_values['experience']),
    gr.Dropdown(label="Project Domain", choices=unique_values['projectDomain']),
    gr.Dropdown(label="Main Specialization", choices=unique_values['mainSpecialization']),
    gr.Dropdown(label="English Proficiency", choices=unique_values['englishProficiency']),
    gr.Dropdown(label="Company Size UA", choices=unique_values['companySizeUA']),
    gr.Dropdown(label="Education Level", choices=unique_values['educationLevel']),
    gr.Dropdown(label="Gender", choices=unique_values['gender'])
]

output = gr.Textbox(label="Predicted Salary")

interface = gr.Interface(fn=predict_salary, inputs=inputs, outputs=output, title="Salary Prediction")
interface.launch()

"""#Randomized Search for  Random Forest Optimization (uncomment to use)"""

# from sklearn.model_selection import RandomizedSearchCV
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import mean_squared_error, r2_score

# # Define the parameter grid
# param_dist = {
#     'n_estimators': [100, 200, 300, 400, 500],
#     'max_depth': [10, 20, 30, 40, None],
#     'max_features': ['auto', 'sqrt', 'log2', 0.5],
#     'min_samples_split': [2, 5, 10, 20],
#     'min_samples_leaf': [1, 2, 4, 10]
# }

# # Create RandomizedSearchCV object
# random_search = RandomizedSearchCV(
#     estimator=RandomForestRegressor(random_state=42),
#     param_distributions=param_dist,
#     n_iter=100,  # Number of parameter combinations to try
#     scoring='r2',  # Optimize for R^2 score
#     cv=5,  # 5-fold cross-validation
#     random_state=42,
#     n_jobs=-1  # Use all available CPU cores
# )

# # Fit RandomizedSearchCV
# random_search.fit(X_train, y_train)

# # Get the best parameters
# best_params = random_search.best_params_
# print("Best hyperparameters:", best_params)

# # Use the best model for predictions
# best_rf_model = random_search.best_estimator_
# y_val_pred_rf_optimized = best_rf_model.predict(X_val)

# # Evaluate the optimized model on the validation set
# mse_rf_optimized = mean_squared_error(y_val, y_val_pred_rf_optimized)
# r2_rf_optimized = r2_score(y_val, y_val_pred_rf_optimized)

# print(f"Optimized Mean Squared Error (Validation): {mse_rf_optimized}")
# print(f"Optimized R^2 Score (Validation): {r2_rf_optimized}")

"""# DecisionTreeRegressor"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

dt_model = DecisionTreeRegressor(
    max_depth=10,
    max_features=None,
    min_samples_leaf=15,
    min_samples_split=12,
    random_state=42
)

dt_model.fit(X_train, y_train)

# Prediction for validation data
y_val_pred_dt = dt_model.predict(X_val)

# Evaluation of the model on the validation set
mse_dt = mean_squared_error(y_val, y_val_pred_dt)
r2_dt = r2_score(y_val, y_val_pred_dt)

print(f"Mean Squared Error: {mse_dt}")
print(f"R^2 Score: {r2_dt}")

print("_____________DecisionTreeRegressor___________________")


# Prediction for training data
y_train_pred_dt = dt_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred_dt)
print(f"MSE for training data: {mse_train}")

# Prediction for validation data
y_val_pred_dt = dt_model.predict(X_val)
mse_val = mean_squared_error(y_val, y_val_pred_dt)
print(f"MSE for validation data: {mse_val}")

# Prediction for test data
y_test_pred_dt = dt_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred_dt)
print(f"MSE for test data: {mse_test}")

print("______________________________________")

from sklearn.metrics import r2_score

# Prediction for training data
y_train_pred_dt = dt_model.predict(X_train)
r2_train = r2_score(y_train, y_train_pred_dt)

# Prediction for validation data
y_val_pred_dt = dt_model.predict(X_val)
r2_val = r2_score(y_val, y_val_pred_dt)

# Prediction for test data
y_test_pred_dt = dt_model.predict(X_test)
r2_test = r2_score(y_test, y_test_pred_dt)

print(f"R² Score for training data: {r2_train}")
print(f"R² Score for validation data: {r2_val}")
print(f"R² Score for test data: {r2_test}")

print("______________________________________")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

y_val_pred_dt = dt_model.predict(X_val)

indexs = np.arange(0, len(y_val_pred_dt), 50)

plt.figure(figsize=(10, 5))
plt.plot(indexs, y_val.values[indexs], label='Реальні значення', color='#A066CB')
plt.plot(indexs, y_val_pred_dt[indexs], label='Прогнозовані значення (DecisionTreeRegressor)', color='#FF96A3')

plt.xlabel("Індекс спостереження")
plt.xticks(indexs, rotation='vertical', fontsize=8)
plt.ylabel("Місячна заробітна плата ($)")
plt.title("Порівняння прогнозованих і реальних значень для DecisionTreeRegressor")
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)
plt.show()

# Обчислення метрик
mse_dt = mean_squared_error(y_val, y_val_pred_dt)
r2_dt = r2_score(y_val, y_val_pred_dt)

print(f"Mean Squared Error: {mse_dt}")
print(f"R^2 Score: {r2_dt}")

y_val_pred_dt_original = scaler.inverse_transform(y_val_pred_dt.reshape(-1, 1))
y_val_original = scaler.inverse_transform(y_val.values.reshape(-1, 1))  # Convert Series to NumPy array

errors = abs(y_val_original - y_val_pred_dt_original)

result_df = pd.DataFrame({
    'Predicted Salary': y_val_pred_dt_original.flatten(),
    'Actual Salary': y_val_original.flatten(),
    'Error': errors.flatten()
})

top_15_df = result_df.sort_values(by='Error').head(15)

print(top_15_df)

top_15_predictions = evaluate_model(dt_model, X_val, y_val, scaler)

print(top_15_predictions)

"""#Using Randomized SearchCV for Optimization DecisionTreeRegressor (uncomment to use)"""

# from sklearn.model_selection import RandomizedSearchCV
# from scipy.stats import randint


# param_dist = {
#     'max_depth': [3, 5, 10, None],
#     'min_samples_split': randint(2, 20),
#     'min_samples_leaf': randint(1, 20),
#     'max_features': [None, 'sqrt', 'log2']
# }


# random_search = RandomizedSearchCV(
#     estimator=DecisionTreeRegressor(random_state=42),
#     param_distributions=param_dist,
#     scoring='r2',
#     n_iter=100,  # Number of parameter settings sampled
#     cv=5,
#     random_state=42,
#     n_jobs=-1
# )


# random_search.fit(X_train, y_train)


# print("Best hyperparameters:", random_search.best_params_)
# print("Best R^2 score:", random_search.best_score_)

"""# GradientBoostingRegressor"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score


gb_model = GradientBoostingRegressor(
    subsample=0.8,
    n_estimators=300,
    min_samples_split=20,
    min_samples_leaf=2,
    max_features=None,
    max_depth=7,
    learning_rate=0.05,
    random_state=42
)

gb_model.fit(X_train, y_train)

# Predict on the validation set
y_val_pred_gb = gb_model.predict(X_val)

# Evaluate the model on the validation set
mse_gb = mean_squared_error(y_val, y_val_pred_gb)
r2_gb = r2_score(y_val, y_val_pred_gb)


print(f"Mean Squared Error: {mse_gb}")
print(f"R^2 Score: {r2_gb}")

print("_____________GradientBoostingRegressor___________________")

# Prediction for training data
y_train_pred_dt = gb_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred_dt)
print(f"MSE for training data: {mse_train}")

# Prediction for validation data
y_val_pred_dt = gb_model.predict(X_val)
mse_val = mean_squared_error(y_val, y_val_pred_dt)
print(f"MSE for validation data: {mse_val}")

# Prediction for test data
y_test_pred_dt = gb_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred_dt)
print(f"MSE for test data:: {mse_test}")

print("______________________________________")

from sklearn.metrics import r2_score

# Prediction for training data
y_train_pred_dt = gb_model.predict(X_train)
r2_train = r2_score(y_train, y_train_pred_dt)

# Prediction for validation data
y_val_pred_dt = gb_model.predict(X_val)
r2_val = r2_score(y_val, y_val_pred_dt)

# Prediction for test data
y_test_pred_dt = gb_model.predict(X_test)
r2_test = r2_score(y_test, y_test_pred_dt)

print(f"R² Score for training data: {r2_train}")
print(f"R² Score for validation data: {r2_val}")
print(f"R² Score for test data: {r2_test}")
print("______________________________________")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

y_val_pred_gb = gb_model.predict(X_val)

indexs = np.arange(0, len(y_val_pred_gb), 50)

plt.figure(figsize=(10, 5))
plt.plot(indexs, y_val.values[indexs], label='Реальні значення', color='#A066CB')
plt.plot(indexs, y_val_pred_gb[indexs], label='Прогнозовані значення (GradientBoostingRegressor)', color='#FF96A3')

plt.xlabel("Індекс спостереження")
plt.xticks(indexs, rotation='vertical', fontsize=8)
plt.ylabel("Місячна заробітна плата ($)")
plt.title("Порівняння прогнозованих і реальних значень для GradientBoostingRegressor")
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)
plt.show()

mse_gb = mean_squared_error(y_val, y_val_pred_gb)
r2_gb = r2_score(y_val, y_val_pred_gb)

print(f"Mean Squared Error: {mse_gb}")
print(f"R^2 Score: {r2_gb}")

"""#Using Randomized Search for Optimization GradientBoostingRegressor (uncomment to use)"""

# from sklearn.model_selection import RandomizedSearchCV
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.metrics import mean_squared_error, r2_score
# import numpy as np

# param_dist = {
#     'n_estimators': [50, 100, 200, 300],
#     'learning_rate': [0.01, 0.05, 0.1, 0.2],
#     'max_depth': [3, 5, 7, 10],
#     'min_samples_split': [2, 5, 10, 20],
#     'min_samples_leaf': [1, 2, 5, 10],
#     'subsample': [0.6, 0.8, 1.0],
#     'max_features': [None, 'sqrt', 'log2']
# }

# random_search = RandomizedSearchCV(
#     estimator=GradientBoostingRegressor(random_state=42),
#     param_distributions=param_dist,
#     n_iter=100,  # Number of parameter combinations to try
#     scoring='r2',  # Optimize for R^2 score
#     cv=5,  # 5-fold cross-validation
#     random_state=42,
#     n_jobs=-1  # Use all available cores
# )

# random_search.fit(X_train, y_train)

# best_params = random_search.best_params_
# best_score = random_search.best_score_

# print("Best hyperparameters:", best_params)
# print("Best R^2 score during CV:", best_score)

# best_gb_model = random_search.best_estimator_
# y_val_pred_gb_optimized = best_gb_model.predict(X_val)

# mse_gb_optimized = mean_squared_error(y_val, y_val_pred_gb_optimized)
# r2_gb_optimized = r2_score(y_val, y_val_pred_gb_optimized)

# print(f"Optimized Mean Squared Error: {mse_gb_optimized}")
# print(f"Optimized R^2 Score: {r2_gb_optimized}")

"""# Neural Networks (MLPRegressor)"""

from sklearn.neural_network import MLPRegressor

mlpr_model = MLPRegressor(
    solver='sgd',
    max_iter=500,
    learning_rate_init=0.01,
    hidden_layer_sizes=(100, 100),
    alpha=0.01,
    activation='tanh',
    random_state=42  # For reproducibility
)
mlpr_model.fit(X_train, y_train)

y_val_pred_mlpr = mlpr_model.predict(X_val)

mse_mlpr = mean_squared_error(y_val, y_val_pred_mlpr)
r2_mlpr = r2_score(y_val, y_val_pred_mlpr)

print(f"Mean Squared Error: {mse_mlpr}")
print(f"R^2 Score: {r2_mlpr}")

print("_____________Neural Networks (MLPRegressor)___________________")

y_train_pred_dt = mlpr_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred_dt)
print(f"MSE for taining data: {mse_train}")

y_val_pred_dt = mlpr_model.predict(X_val)
mse_val = mean_squared_error(y_val, y_val_pred_dt)
print(f"MSE for validation data: {mse_val}")

y_test_pred_dt = mlpr_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred_dt)
print(f"MSE for test data: {mse_test}")

print("______________________________________")

from sklearn.metrics import r2_score

y_train_pred_dt = mlpr_model.predict(X_train)
r2_train = r2_score(y_train, y_train_pred_dt)

y_val_pred_dt = mlpr_model.predict(X_val)
r2_val = r2_score(y_val, y_val_pred_dt)

y_test_pred_dt = mlpr_model.predict(X_test)
r2_test = r2_score(y_test, y_test_pred_dt)

print(f"R² Score for training data: {r2_train}")
print(f"R² Score for validation data: {r2_val}")
print(f"R² Score for test data: {r2_test}")
print("______________________________________")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

# Отримуємо прогнозовані значення для валідаційного набору
y_val_pred_mlpr = mlpr_model.predict(X_val)

# Побудова графіка для порівняння прогнозованих і реальних значень
indexs = np.arange(0, len(y_val_pred_mlpr), 50)  # Вибираємо кожне 50-те значення для наочності

plt.figure(figsize=(10, 5))
plt.plot(indexs, y_val.values[indexs], label='Реальні значення', color='#A066CB')
plt.plot(indexs, y_val_pred_mlpr[indexs], label='Прогнозовані значення (MLPRegressor)', color='#FF96A3')

plt.xlabel("Індекс спостереження")
plt.xticks(indexs, rotation='vertical', fontsize=8)
plt.ylabel("Місячна заробітна плата ($)")
plt.title("Порівняння прогнозованих і реальних значень для MLPRegressor")
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)
plt.show()

# Обчислення метрик
mse_mlpr = mean_squared_error(y_val, y_val_pred_mlpr)
r2_mlpr = r2_score(y_val, y_val_pred_mlpr)

print(f"Mean Squared Error: {mse_mlpr}")
print(f"R^2 Score: {r2_mlpr}")

"""# VotingRegressor"""

from sklearn.ensemble import VotingRegressor

voting_model = VotingRegressor([
    ('rf', rf_model),
    ('gb', gb_model),
    # ('dt', dt_model),
])

voting_model.fit(X_train, y_train)

y_val_pred_voting = voting_model.predict(X_val)

mse_voting = mean_squared_error(y_val, y_val_pred_voting)
r2_voting = r2_score(y_val, y_val_pred_voting)

print(f"Mean Squared Error: {mse_voting}")
print(f"R^2 Score: {r2_voting}")

print("_____________VotingRegressor___________________")

y_train_pred_dt = voting_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred_dt)
print(f"MSE for training data:: {mse_train}")

y_val_pred_dt = voting_model.predict(X_val)
mse_val = mean_squared_error(y_val, y_val_pred_dt)
print(f"MSE for validation data: {mse_val}")

y_test_pred_dt = voting_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred_dt)
print(f"MSE for test data: {mse_test}")

print("______________________________________")

from sklearn.metrics import r2_score

y_train_pred_dt = voting_model.predict(X_train)
r2_train = r2_score(y_train, y_train_pred_dt)

y_val_pred_dt = voting_model.predict(X_val)
r2_val = r2_score(y_val, y_val_pred_dt)

y_test_pred_dt = voting_model.predict(X_test)
r2_test = r2_score(y_test, y_test_pred_dt)

print(f"R² Score for training data: {r2_train}")
print(f"R² Score for validation data: {r2_val}")
print(f"R² Score for test data: {r2_test}")
print("______________________________________")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

y_val_pred_voting = voting_model.predict(X_val)

indexs = np.arange(0, len(y_val_pred_voting), 50)

plt.figure(figsize=(10, 5))
plt.plot(indexs, y_val.values[indexs], label='Реальні значення', color='#A066CB')
plt.plot(indexs, y_val_pred_voting[indexs], label='Прогнозовані значення (VotingRegressor)', color='#FF96A3')

plt.xlabel("Індекс спостереження")
plt.xticks(indexs, rotation='vertical', fontsize=8)
plt.ylabel("Місячна заробітна плата ($)")
plt.title("Порівняння прогнозованих і реальних значень для VotingRegressor")
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)
plt.show()

mse_voting = mean_squared_error(y_val, y_val_pred_voting)
r2_voting = r2_score(y_val, y_val_pred_voting)

print(f"Mean Squared Error: {mse_voting}")
print(f"R^2 Score: {r2_voting}")

"""# SVM (Support Vector Machines)"""

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)
svr_model.fit(X_train_scaled, y_train)

y_val_pred_svr = svr_model.predict(X_val_scaled)

y_test_pred_svr = svr_model.predict(X_test_scaled)

mse_svr = mean_squared_error(y_test, y_test_pred_svr)
r2_svr = r2_score(y_test, y_test_pred_svr)

print(f"Mean Squared Error (SVR): {mse_svr}")
print(f"R^2 Score (SVR): {r2_svr}")
print("_____________Support Vector Regressor___________________")

y_train_pred_svr = svr_model.predict(X_train_scaled)
mse_train_svr = mean_squared_error(y_train, y_train_pred_svr)
print(f"MSE for training data (SVR): {mse_train_svr}")

y_val_pred_svr = svr_model.predict(X_val_scaled)
mse_val_svr = mean_squared_error(y_val, y_val_pred_svr)
print(f"MSE for validation data (SVR): {mse_val_svr}")

y_test_pred_svr = svr_model.predict(X_test_scaled)
mse_test_svr = mean_squared_error(y_test, y_test_pred_svr)
print(f"MSE for test data: (SVR) {mse_test_svr}")

print("______________________________________")

r2_train_svr = r2_score(y_train, y_train_pred_svr)
r2_val_svr = r2_score(y_val, y_val_pred_svr)
r2_test_svr = r2_score(y_test, y_test_pred_svr)

print(f"R² Score for training data (SVR): {r2_train_svr}")
print(f"R² Score for validation data (SVR): {r2_val_svr}")
print(f"R² Score for test data (SVR): {r2_test_svr}")

import numpy as np
import matplotlib.pyplot as plt

y_val_pred_svr = svr_model.predict(X_val_scaled)

indexs = np.arange(0, len(y_val_pred_svr), 50)

plt.figure(figsize=(10, 5))
plt.plot(indexs, y_val.values[indexs], label='Реальні значення', color='#A066CB')
plt.plot(indexs, y_val_pred_svr[indexs], label='Прогнозовані значення (SVR)', color='#FF96A3')

plt.xlabel("Індекс спостереження")
plt.xticks(indexs, rotation='vertical', fontsize=8)
plt.ylabel("Місячна заробітна плата ($)")
plt.title("Порівняння прогнозованих і реальних значень для SVR")
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)
plt.show()

"""# GridSearchCV SVR()"""

# from sklearn.svm import SVR
# from sklearn.model_selection import GridSearchCV
# from sklearn.preprocessing import StandardScaler

# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_val_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# param_grid = {
#     'kernel': ['rbf', 'linear'],
#     'C': [0.1, 1, 10],
#     'epsilon': [0.01, 0.1, 0.2]
# }

# grid_search = GridSearchCV(
#     estimator=SVR(),
#     param_grid=param_grid,
#     cv=5,
#     scoring='r2',
#     verbose=1,  # To show progress
#     n_jobs=-1   # Use all available CPU cores
# )

# grid_search.fit(X_train_scaled, y_train)

# best_svr_model = grid_search.best_estimator_
# best_params = grid_search.best_params_
# best_score = grid_search.best_score_

# print("Найкращі параметри:", best_params)
# print("Найкращий R² Score (середнє значення на крос-валідації):", best_score)

# y_val_pred_svr = best_svr_model.predict(X_val_scaled)
# r2_val = r2_score(y_val, y_val_pred_svr)
# print(f"R² Score на валідаційних даних: {r2_val}")

# y_test_pred_svr = best_svr_model.predict(X_test_scaled)
# r2_test = r2_score(y_test, y_test_pred_svr)
# print(f"R² Score на тестових даних: {r2_test}")

# from sklearn.model_selection import RandomizedSearchCV
# from scipy.stats import uniform, loguniform
# from sklearn.svm import SVR

# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_val_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# param_dist = {
#    'kernel': ['rbf', 'linear'],
#     'C': [0.1, 1, 10],
#     'epsilon': [0.01, 0.1, 0.2]
# }

# random_search = RandomizedSearchCV(
#     estimator=SVR(),
#     param_distributions=param_dist,
#     n_iter=10,  # Кількість випадкових комбінацій
#     cv=5,
#     scoring='r2',
#     verbose=1,
#     n_jobs=-1,
#     random_state=42
# )

# random_search.fit(X_train_scaled, y_train)

# best_svr_model = random_search.best_estimator_
# best_params = random_search.best_params_
# best_score = random_search.best_score_

# print("Найкращі параметри:", best_params)
# print("Найкращий R² Score (середнє значення на крос-валідації):", best_score)

# y_val_pred_svr = best_svr_model.predict(X_val_scaled)
# r2_val = r2_score(y_val, y_val_pred_svr)
# print(f"R² Score на валідаційних даних: {r2_val}")

# y_test_pred_svr = best_svr_model.predict(X_test_scaled)
# r2_test = r2_score(y_test, y_test_pred_svr)
# print(f"R² Score на тестових даних: {r2_test}")

"""# XBoost"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

xgb_model = XGBRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0.1,
    random_state=42
)

xgb_model.fit(X_train, y_train)

y_train_pred_xgb = xgb_model.predict(X_train)
mse_train_xgb = mean_squared_error(y_train, y_train_pred_xgb)
r2_train_xgb = r2_score(y_train, y_train_pred_xgb)

y_val_pred_xgb = xgb_model.predict(X_val)
mse_val_xgb = mean_squared_error(y_val, y_val_pred_xgb)
r2_val_xgb = r2_score(y_val, y_val_pred_xgb)

y_test_pred_xgb = xgb_model.predict(X_test)
mse_test_xgb = mean_squared_error(y_test, y_test_pred_xgb)
r2_test_xgb = r2_score(y_test, y_test_pred_xgb)

print("________________XGBoost______________________")

print(f"MSE for training data (XGBoost): {mse_train_xgb}")
print(f"MSE for validation data (XGBoost): {mse_val_xgb}")
print(f"MSE for test data (XGBoost): {mse_test_xgb}")

print("______________________________________")

print(f"R² Score for training data (XGBoost): {r2_train_xgb}")
print(f"R² Score for validation data (XGBoost): {r2_val_xgb}")
print(f"R² Score for test data (XGBoost): {r2_test_xgb}")

"""# XGBRegressor GridSearch"""

# from xgboost import XGBRegressor
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import mean_squared_error, r2_score

# xgb_model = XGBRegressor(random_state=42)

# param_grid = {
#     'n_estimators': [100, 200, 300],  # Number of boosting rounds
#     'max_depth': [3, 5, 7],           # Maximum depth of a tree
#     'learning_rate': [0.01, 0.1, 0.2], # Step size shrinkage
#     'subsample': [0.8, 1.0],          # Subsample ratio of the training instances
#     'colsample_bytree': [0.8, 1.0],   # Subsample ratio of columns when constructing each tree
#     'gamma': [0, 0.1, 0.2]            # Minimum loss reduction required to make a split
# }

# grid_search = GridSearchCV(
#     estimator=xgb_model,
#     param_grid=param_grid,
#     cv=5,
#     scoring='r2',
#     verbose=1,  # To show progress
#     n_jobs=-1   # Use all available CPU cores
# )

# grid_search.fit(X_train, y_train)

# best_xgb_model = grid_search.best_estimator_
# best_params = grid_search.best_params_
# best_score = grid_search.best_score_

# print("Найкращі параметри:", best_params)
# print("Найкращий R² Score (середнє значення на крос-валідації):", best_score)

# # Evaluate the best model on the training set
# y_train_pred_xgb = best_xgb_model.predict(X_train)
# mse_train_xgb = mean_squared_error(y_train, y_train_pred_xgb)
# r2_train_xgb = r2_score(y_train, y_train_pred_xgb)
# print(f"MSE на тренувальних даних (XGBoost): {mse_train_xgb}")
# print(f"R² Score на тренувальних даних (XGBoost): {r2_train_xgb}")

# # Evaluate the best model on the validation set
# y_val_pred_xgb = best_xgb_model.predict(X_val)
# mse_val_xgb = mean_squared_error(y_val, y_val_pred_xgb)
# r2_val_xgb = r2_score(y_val, y_val_pred_xgb)
# print(f"MSE на валідаційних даних (XGBoost): {mse_val_xgb}")
# print(f"R² Score на валідаційних даних (XGBoost): {r2_val_xgb}")

# # Evaluate the best model on the test set
# y_test_pred_xgb = best_xgb_model.predict(X_test)
# mse_test_xgb = mean_squared_error(y_test, y_test_pred_xgb)
# r2_test_xgb = r2_score(y_test, y_test_pred_xgb)
# print(f"MSE на тестових даних (XGBoost): {mse_test_xgb}")
# print(f"R² Score на тестових даних (XGBoost): {r2_test_xgb}")

"""#StackingRegressor"""

from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor

estimators = [
    ('rf', rf_model),
    ('gb', gb_model),
    # ('dt', dt_model),
    # ('xgb', XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)),
    # ('rf', RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)),
    # ('mlp', MLPRegressor(solver='sgd', max_iter=500, learning_rate_init=0.01, hidden_layer_sizes=(100, 100), alpha=0.01, activation='tanh', random_state=42))
]

stacking_model = StackingRegressor(estimators=estimators, final_estimator=rf_model)
stacking_model.fit(X_train, y_train)

y_train_pred = stacking_model.predict(X_train)
y_val_pred = stacking_model.predict(X_val)
y_test_pred = stacking_model.predict(X_test)

print("_____________Stacking Regressor___________________")
print(f"MSE for training data: {mean_squared_error(y_train, y_train_pred)}")
print(f"MSE for validation data: {mean_squared_error(y_val, y_val_pred)}")
print(f"MSE for test data: {mean_squared_error(y_test, y_test_pred)}")
print(f"R² Score for training data: {r2_score(y_train, y_train_pred)}")
print(f"R² Score for validation data: {r2_score(y_val, y_val_pred)}")
print(f"R² Score for test data: {r2_score(y_test, y_test_pred)}")
print("______________________________________")

# from lightgbm import LGBMRegressor

# lgb_model = LGBMRegressor(
#     n_estimators=200,
#     learning_rate=0.1,
#     max_depth=5,
#     random_state=42
# )

# lgb_model.fit(X_train, y_train)

# y_train_pred = lgb_model.predict(X_train)
# y_val_pred = lgb_model.predict(X_val)
# y_test_pred = lgb_model.predict(X_test)

# print("_____________LightGBM Regressor___________________")
# print(f"MSE for training data: {mean_squared_error(y_train, y_train_pred)}")
# print(f"MSE for validation data: {mean_squared_error(y_val, y_val_pred)}")
# print(f"MSE for test data: {mean_squared_error(y_test, y_test_pred)}")
# print(f"R² Score for training data: {r2_score(y_train, y_train_pred)}")
# print(f"R² Score for validation data: {r2_score(y_val, y_val_pred)}")
# print(f"R² Score for test data: {r2_score(y_test, y_test_pred)}")
# print("______________________________________")

# from sklearn.linear_model import LinearRegression
# from sklearn.preprocessing import PolynomialFeatures

# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_train_poly = poly.fit_transform(X_train)
# X_val_poly = poly.transform(X_val)
# X_test_poly = poly.transform(X_test)

# lr_model = LinearRegression()
# lr_model.fit(X_train_poly, y_train)

# y_train_pred = lr_model.predict(X_train_poly)
# y_val_pred = lr_model.predict(X_val_poly)
# y_test_pred = lr_model.predict(X_test_poly)

# print("_____________Linear Regression with Polynomial Features___________________")
# print(f"MSE for training data: {mean_squared_error(y_train, y_train_pred)}")
# print(f"MSE for validation data: {mean_squared_error(y_val, y_val_pred)}")
# print(f"MSE for test data: {mean_squared_error(y_test, y_test_pred)}")
# print(f"R² Score for training data: {r2_score(y_train, y_train_pred)}")
# print(f"R² Score for validation data: {r2_score(y_val, y_val_pred)}")
# print(f"R² Score for test data: {r2_score(y_test, y_test_pred)}")
# print("______________________________________")